{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08adf2f7-e1aa-4a4a-8376-423da3c553fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaire/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8acf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text: str):\n",
    "    # Webservice call\n",
    "    analyzer_query = \"https://api.tartunlp.ai/grammar\"\n",
    "    request_body = {\n",
    "                      \"language\": \"et\",\n",
    "                      \"text\": text\n",
    "                    }\n",
    "    response = requests.post(analyzer_query, json=request_body)\n",
    "    assert response.ok, \"Webservice failed\"\n",
    "    response = response.json()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcceb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsamples = pd.read_csv(\"obl_gpt_input_large1.csv\", sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4f40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for i in range(len(testsamples)):\n",
    "    sent = testsamples.iloc[i][\"short_sent_untoken\"]\n",
    "    answ = analyze_text(sent)\n",
    "    answers.append(answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5c817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsamples['gpt_answers'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b5ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_cat = []\n",
    "for answ in answers:\n",
    "    if len(answ[\"corrections\"])==0:\n",
    "        gpt_cat.append(\"yes\")\n",
    "    else:\n",
    "        gpt_cat.append(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6d5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsamples['gpt'] = gpt_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a7b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.535 0.537]\n",
      "recall: [0.546 0.526]\n",
      "fscore: [0.541 0.531]\n",
      "support: [500 500]\n",
      "precision:  0.537\n",
      "recall:  0.526\n"
     ]
    }
   ],
   "source": [
    "predicted = list(testsamples['gpt'])\n",
    "y_test = list(testsamples['removetype'])\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision.round(3)))\n",
    "print('recall: {}'.format(recall.round(3)))\n",
    "print('fscore: {}'.format(fscore.round(3)))\n",
    "print('support: {}'.format(support.round(3)))\n",
    "\n",
    "try:\n",
    "    precision = precision_score(y_test, predicted, pos_label='yes', average='binary')\n",
    "    recall = recall_score(y_test, predicted, pos_label='yes', average='binary')\n",
    "except:\n",
    "    precision = precision_score(y_test, predicted, labels=['yes'], average='micro')\n",
    "    recall = recall_score(y_test, predicted, labels=['yes'], average='micro')\n",
    "print('precision: ',precision.round(3))\n",
    "print('recall: ',recall.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f4554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsamples[\"match\"] = testsamples.removetype.eq(testsamples.gpt) # true if annotation and prediction are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0fed86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match\n",
       "True     536\n",
       "False    464\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsamples.value_counts(\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129241a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt\n",
       "no     510\n",
       "yes    490\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsamples.value_counts(\"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2554b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b1d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsamples.to_csv(\"obl_grammar_input_large1_untoken_answers.csv\", index= False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38c67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
