{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e73cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from estnltk.converters.conll.conll_importer import conll_to_text\n",
    "from estnltk_core.layer_operations import split_by_sentences\n",
    "import estnltk\n",
    "from estnltk import Text\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ce538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis on fraasi peasõna ja verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b68d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peasona_and_verb(input_file, sent, phrase):\n",
    "    text_obj = conll_to_text( input_file, syntax_layer='stanza_syntax' )\n",
    "    texts2 = split_by_sentences(text=text_obj,\n",
    "                                   layers_to_keep=list(text_obj.layers),\n",
    "                                   trim_overlapping=True\n",
    "                                   )\n",
    "    # target text object for the sentence\n",
    "    target = None\n",
    "    target_idx = None\n",
    "    for i,txt in enumerate(texts2):\n",
    "        #clean_txt = clean(txt.text)\n",
    "        if txt.text == sent:\n",
    "            target = txt\n",
    "            target_idx = i\n",
    "    if target_idx is None:\n",
    "        for i,txt in enumerate(texts2):\n",
    "            clean_txt = clean(txt.text)\n",
    "            clean_sent = clean(sent)\n",
    "            if clean_txt == clean_sent:\n",
    "                target = txt\n",
    "                target_idx = i\n",
    "    \n",
    "    # target layer\n",
    "    target_layer = texts2[target_idx][\"stanza_syntax\"]\n",
    "    #display(target_layer)\n",
    "    \n",
    "    # words in the removed phrase\n",
    "    phrase_words = {}\n",
    "    for word in phrase.split(\" \"):\n",
    "        phrase_words[word] = None\n",
    "    # and corresponding indexes for the phrase words\n",
    "    for idx, span in enumerate(target_layer):\n",
    "        if span.text in phrase_words.keys():\n",
    "            phrase_words[span.text] = idx\n",
    "    # heads for the phrase words\n",
    "    phrase_words_heads = {}\n",
    "    for word in phrase_words.keys():\n",
    "        if phrase_words[word] is not None: \n",
    "            # \"Stuart Little 79 398\" numbrid on ühe sõnana aga kuna siin on need tühiku alusel lahku löödud\n",
    "            # siis jääb nende asemel None ja võimalik, et ei leita verbi\n",
    "            phrase_words_heads[word] = target_layer[phrase_words[word]].head   \n",
    "    # word with the smallest head is the main word in phrase\n",
    "    peasona = min(phrase_words_heads, key=phrase_words_heads.get)\n",
    "    peasona_idx = phrase_words[peasona]\n",
    "    peasonahead = phrase_words_heads[peasona]\n",
    "    \n",
    "    ## verbi otsimine\n",
    "    phrase_verb = None\n",
    "    isverb = False\n",
    "    if peasonahead != 0:\n",
    "        # peasona on verb\n",
    "        if target_layer[peasona_idx].xpostag == \"V\":\n",
    "            #print(\"peasona on verb\")\n",
    "            phrase_verb = peasona_idx\n",
    "            isverb = True\n",
    "\n",
    "        # peasona head on verb\n",
    "        elif target_layer[peasonahead-1].xpostag == \"V\":\n",
    "            phrase_verb = peasonahead\n",
    "            isverb = True\n",
    "            #print(\"head on verb\", phrase_verb)\n",
    "\n",
    "        # peasona head ei ole verb, tuleb hakata ülespoole liikuma\n",
    "        elif target_layer[peasonahead-1].xpostag != \"V\":\n",
    "            new_head = peasonahead\n",
    "            phrase_verb = None\n",
    "            while not isverb:\n",
    "                #print(\"peasõna head\", new_head, \"ehk \", target_layer[new_head-1])\n",
    "                if target_layer[new_head-1].xpostag == \"V\":\n",
    "                    isverb = True\n",
    "                    phrase_verb = new_head\n",
    "                else:\n",
    "                    new_head = target_layer[new_head-1].head\n",
    "                    if new_head == 0 and isverb==False: # kui jõudsime rootini aga verbi ei ole\n",
    "                        break\n",
    "\n",
    "    # fraasi peasõna ei ole verbi alluv, võta siis esimene verb\n",
    "    if not isverb or peasonahead == 0:\n",
    "        for span in target_layer:\n",
    "            if span.upostag == \"VERB\" or span.xpostag == \"V\":\n",
    "                phrase_verb = span.id\n",
    "                break\n",
    "    \n",
    "    verb = None\n",
    "    if phrase_verb is not None:\n",
    "        verb = target_layer[phrase_verb-1].text\n",
    "    \n",
    "    \n",
    "    # return peasõna, peasõna index, peasõna parent ehk head, verb, verbi id\n",
    "    return peasona, peasona_idx, peasonahead, verb, phrase_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a3578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deprels = [\"acl\", \"advcl\", \"advmod\", \"appos\", \"det\", \"nmod\", \"nummod\", \"obl\"] # \"discourse\", \"vocative\"\n",
    "\n",
    "root = r\".../UDpuupank/UD2_11_udreposse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef4a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deprel = deprels[3] \n",
    "# nummod [6] on probleemne \n",
    "# phrase_words_heads[word] = target_layer\n",
    "# TypeError: index not supported: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2f206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [1:24:29<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "print(deprel)\n",
    "data = pd.read_csv(f\"data/{deprel}_benchmark_1000.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "free_sent_data = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    #if i == 441:\n",
    "    sent = data.iloc[i][\"sentence\"]\n",
    "    phrase = data.iloc[i][\"removed\"]\n",
    "    file = data.iloc[i][\"fpath\"]\n",
    "    #print(sent)\n",
    "    input_file = os.path.join(root, file) \n",
    "    free_sent_data.append((get_peasona_and_verb(input_file, sent, phrase)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24685d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentence_analysis_data\"] = free_sent_data\n",
    "free_peasonad = []\n",
    "free_verbs = []\n",
    "\n",
    "for data2 in free_sent_data:\n",
    "    free_peasonad.append(data2[0])\n",
    "    free_verbs.append(data2[3])\n",
    "\n",
    "data[\"peasona\"] = free_peasonad\n",
    "data[\"verb\"] = free_verbs\n",
    "\n",
    "data.to_csv(f\"data/{deprel}_benchmark_1000_peasona_verb.csv\", index=False, sep=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6a4499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
