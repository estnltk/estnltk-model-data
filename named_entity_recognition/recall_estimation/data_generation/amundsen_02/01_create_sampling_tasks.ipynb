{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5902d852",
   "metadata": {},
   "source": [
    "# Workflow for searching new recall samples\n",
    "\n",
    "Finds a random sample of potential positive samples. Converts these to labelstudio format for later manual tagging. \n",
    "\n",
    "For this specific task, we create subsamples based on partofspeech tags of words preceding geographical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f1b79",
   "metadata": {},
   "source": [
    "## I. Setup\n",
    "\n",
    "### Loading the source corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f075bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:storage.py:57: connecting to host: 'postgres.keeleressursid.ee', port: 5432, dbname: 'estonian-text-corpora', user: 'soras'\n",
      "INFO:storage.py:108: schema: 'estonian_text_corpora', temporary: False, role: 'estonian_text_corpora_read'\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import load_configuration, connect_to_database\n",
    "\n",
    "config = load_configuration('config\\example_configuration.ini')\n",
    "storage = connect_to_database(config)\n",
    "\n",
    "collection = config['source_database']['collection']\n",
    "collection = storage[collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a04c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.selected_layers = ['v171_named_entities', 'v172_geo_terms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a6479",
   "metadata": {},
   "source": [
    "Read the geographical terms (extracted from the WordNet) that can be a part of a named geographical entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e8cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 terms.\n"
     ]
    }
   ],
   "source": [
    "terms = []\n",
    "with open('geo_terms.txt', 'r', encoding='UTF-8') as in_f:\n",
    "    for line in in_f:\n",
    "        if len(line.strip()) > 0:\n",
    "            terms.append( line.strip() )\n",
    "print(f'Loaded {len(terms)} terms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578602c",
   "metadata": {},
   "source": [
    "### Local database for sampling\n",
    "\n",
    "Initialize SpanSampler that uses a local sqlite database for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d13752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_sampling_db:  geo_terms_pos_sample.db (exists: True)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from helper_functions import load_local_configuration\n",
    "\n",
    "config = load_local_configuration('config/example_configuration.ini')\n",
    "sampling_db = config['local_database']['sqlite_file']\n",
    "print(f'local_sampling_db:  {sampling_db} (exists: {os.path.exists(sampling_db)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2945cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 terms from geo_terms.txt.\n"
     ]
    }
   ],
   "source": [
    "from span_sampler_sqlite3 import SpanSampler\n",
    "\n",
    "sampler = SpanSampler(collection=collection, \n",
    "                      layer='v172_geo_terms', \n",
    "                      attribute='lemma', \n",
    "                      termsfile='geo_terms.txt', \n",
    "                      db_file_name=config['local_database']['sqlite_file'], \n",
    "                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420bfd80",
   "metadata": {},
   "source": [
    "Before sampling, all terms that are subject sampling need to be searched from the large source database and recorded into a  local database, so that sampling can be quick and smooth. This searching and indexing process can take several hours. \n",
    "\n",
    "**Note:** If you already have the local database populated with term locations, then the next command produces a warning and skips the local database creation. If you still want to repeat the local database creation from the scratch, then you should delete the local database file before creating SpanSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf114abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Kraam\\estnltk_1.7_aux\\estnltk-model-data\\named_entity_recognition\\recall_estimation\\data_generation\\amundsen_02\\span_sampler_sqlite3.py:75: UserWarning: (!) 'geo_terms_pos_sample.db' already contains attribute_locations_pos table. Skipping the table creation.\n",
      "  warnings.warn(f'(!) {self._db_file_name!r} already contains attribute_locations_pos table. Skipping the table creation.')\n"
     ]
    }
   ],
   "source": [
    "# build terms index (can take several hours if done from the scratch)\n",
    "sampler.create_attribute_locations_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f16d71",
   "metadata": {},
   "source": [
    "Once the indexing has been completed, we can start sampling from the local database. \n",
    "We can create the samples by calling the sampler, specifying the count of samples we want and a filter which is a list of attribute values: partofspeech tags of words preceding geographical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef16de",
   "metadata": {},
   "source": [
    "For testing: lets sample geographical terms preceded by different types of adjectives (\"A\", \"C\", \"U\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0232d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Text(text='“ Tallinna vanalinnas mistahes tehinguid tehes tuleb arvestada kümneprotsendise altkäemaksuga , ” kinnitavad Luubi hästiinformeeritud allikad .'),\n",
       "  (54915, 115, 141, 'A')),\n",
       " (Text(text='Taas jookseb kuum juga üle selja , käed tõmbuvad higiseks , süda klopib .'),\n",
       "  (55615, 13, 22, 'A')),\n",
       " (Text(text='Mõne saare või terve mandri ( Atlantise ? ) merrevajumisest veelgi rohkem võib kaasaja inimese läbi raputada hoopiski tsivilisatsiooni hukk “ otse meie silme all ” .'),\n",
       "  (56776, 15, 27, 'A'))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples_adjectives = sampler(count=1000, attribute_values=(\"A\", \"C\", \"U\"))\n",
    "display(samples_adjectives[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55af18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><div align = \"left\">“ Tallinna vanalinnas mistahes tehinguid tehes tuleb arvestada kümneprotsendise altkäemaksuga , ” kinnitavad Luubi hästiinformeeritud allikad .</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<h4>Metadata</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>file</td>\n",
       "      <td>aja_luup_1998_06.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sent_end</td>\n",
       "      <td>6319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sent_start</td>\n",
       "      <td>6176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>subcorpus</td>\n",
       "      <td>aja_luup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>text_no</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>title</td>\n",
       "      <td>Tallinna vanalinna viimsed päevad Vanalinna 1510 hoonet ähvardab häving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>type</td>\n",
       "      <td>artikkel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>v172_geo_terms</td>\n",
       "      <td>lemma</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Text(text='“ Tallinna vanalinnas mistahes tehinguid tehes tuleb arvestada kümneprotsendise altkäemaksuga , ” kinnitavad Luubi hästiinformeeritud allikad .')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_adjectives[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802fcd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "\n",
       "\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>v172_geo_terms</td>\n",
       "      <td>lemma</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>allikad</td>\n",
       "      <td>allikas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name='v172_geo_terms', attributes=('lemma',), spans=SL[Span('allikad', [{'lemma': 'allikas'}])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_adjectives[0][0]['v172_geo_terms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0a40d",
   "metadata": {},
   "source": [
    "## II. Creating unlabelled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1aaffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from copy import copy\n",
    "from estnltk.converters.label_studio.label_studio import LabelStudioExporter\n",
    "\n",
    "output_dir = 'unlabelled/pos_terms_1000'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Take samples for all partofspeech:\n",
    "# https://github.com/estnltk/estnltk/blob/main/tutorials/nlp_pipeline/B_morphology/00_tables_of_morphological_categories.ipynb\n",
    "# Note that there is a redundancy: not all postags can be inside a named entity phrase\n",
    "for partofspeech in ('A','C','D','G','H','I','J','K','N','O','P','S','U','V','X','Y','Z'):\n",
    "    samples = sampler(count=1000, attribute_values=\"('\"+partofspeech+\"')\")\n",
    "\n",
    "    for text, sample_span in samples:\n",
    "        spanstart = sample_span[1]\n",
    "        spanend = sample_span[2]\n",
    "        # Remove geo terms spans that are not preceded by the given postag\n",
    "        for span in copy(text.v172_geo_terms.spans):\n",
    "            if span.start != spanstart and span.end != spanend:\n",
    "                text.v172_geo_terms.remove_span(span)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, \"pos_\"+partofspeech+\"_1000.json\")\n",
    "    exporter = LabelStudioExporter(output_path, ['v172_geo_terms'], checkbox=True)\n",
    "\n",
    "    only_texts = [sample[0] for sample in samples]\n",
    "\n",
    "    exporter.convert(only_texts, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293eb37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <View>\n",
      "            <Labels name=\"label\" toName=\"text\">\n",
      "\t<Label value=\"v172_geo_terms\" background=\"#04DA21\"/> \n",
      "\n",
      "            </Labels>\n",
      "        <Text name=\"text\" value=\"$text\"/>\n",
      "            \n",
      "            </View>\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(output_dir, \"pos_\"+partofspeech+\"_1000.json\")\n",
    "exporter = LabelStudioExporter(output_path, ['v172_geo_terms'], checkbox=True)\n",
    "print(exporter.interface_generator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
