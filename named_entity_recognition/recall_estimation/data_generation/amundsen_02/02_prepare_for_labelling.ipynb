{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c7e697",
   "metadata": {},
   "source": [
    "# Prepare sampled data for manual labelling\n",
    "\n",
    "## I. Extend spans to two word phrases [a postfix]\n",
    "\n",
    "Spans sampled from the database cover only geographical terms. Extend these to cover also a word preceding the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e02fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "input_dir = 'unlabelled/pos_terms_1000'\n",
    "assert os.path.exists(input_dir), \\\n",
    "    f'(!) Missing input dir {input_dir!r}. Please run \"01_create_sampling_tasks.ipynb\" before running this.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c8f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_A_1000.json\n",
      "975\n",
      "975\n",
      "pos_C_1000.json\n",
      "819\n",
      "819\n",
      "pos_D_1000.json\n",
      "965\n",
      "965\n",
      "pos_G_1000.json\n",
      "864\n",
      "864\n",
      "pos_H_1000.json\n",
      "991\n",
      "991\n",
      "pos_I_1000.json\n",
      "198\n",
      "198\n",
      "pos_J_1000.json\n",
      "976\n",
      "976\n",
      "pos_K_1000.json\n",
      "941\n",
      "941\n",
      "pos_N_1000.json\n",
      "877\n",
      "877\n",
      "pos_O_1000.json\n",
      "556\n",
      "556\n",
      "pos_P_1000.json\n",
      "979\n",
      "979\n",
      "pos_S_1000.json\n",
      "993\n",
      "993\n",
      "pos_U_1000.json\n",
      "687\n",
      "687\n",
      "pos_V_1000.json\n",
      "990\n",
      "990\n",
      "pos_X_1000.json\n",
      "25\n",
      "25\n",
      "pos_Y_1000.json\n",
      "799\n",
      "799\n",
      "pos_Z_1000.json\n",
      "953\n",
      "953\n"
     ]
    }
   ],
   "source": [
    "# This runs on newly generated data\n",
    "output_dir = 'unlabelled/pos_terms_1000_extended'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Finds last word (an unit separated by whitespace) in a string\n",
    "def lastWord(string):\n",
    "    string = \" \" + string\n",
    "    # taking empty string\n",
    "    newstring = \"\"\n",
    "    # calculating length of string\n",
    "    length = len(string)\n",
    "    # traversing from last\n",
    "    for i in range(length-1, 0, -1):\n",
    "        # if space is occurred then return\n",
    "        if(string[i] == \" \"):\n",
    "            # return reverse of newstring\n",
    "            return newstring[::-1]\n",
    "        else:\n",
    "            newstring = newstring + string[i]\n",
    "    return newstring[::-1]\n",
    "\n",
    "# Extend all spans to cover one preceding word\n",
    "c = 0\n",
    "for file in os.listdir(input_dir):\n",
    "    print(file)\n",
    "    with open(f\"{input_dir}/{file}\", 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(len(data))\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i]['data']['text']\n",
    "        try:\n",
    "            old_start = data[i]['predictions'][0]['result'][0]['value']['start']\n",
    "            old_end  = data[i]['predictions'][0]['result'][0]['value']['end']\n",
    "            old_geo  = data[i]['predictions'][0]['result'][0]['value']['text']\n",
    "            if old_start != 0:\n",
    "                #print(sentence, old_geo)\n",
    "                prev_word = lastWord(sentence[:old_start].strip())\n",
    "                len_prev_word = len(prev_word)\n",
    "                #print(prev_word)\n",
    "                #print()\n",
    "                new_start = (old_start - len_prev_word) -1 if (old_start - len_prev_word)-1 >=0 else 0\n",
    "                #print(prev_word)\n",
    "                new_geo = prev_word + \" \" + old_geo\n",
    "                data[i]['predictions'][0]['result'][0]['value']['start'] = new_start\n",
    "                data[i]['predictions'][0]['result'][0]['value']['text'] = new_geo\n",
    "                #data[i]['predictions'][0]['result'] = data[i]['predictions'][0]['result'][0]\n",
    "            if len(data[i]['predictions'][0]['result']) > 1:\n",
    "                c+= 1\n",
    "        except Exception as e:\n",
    "            # old_start = data[i]['predictions'][0]['result'][0]['value']['start']\n",
    "            # IndexError: list index out of range \n",
    "            # --> Some wierd problems with indexes. Rasmus and Kaire will know the details\n",
    "            continue\n",
    "    print(len(data))\n",
    "    filename_stub = file.replace(\".json\", \"\")\n",
    "    output_path = os.path.join(output_dir, f'{filename_stub}_extended.json')\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a706d",
   "metadata": {},
   "source": [
    "## II. Pick 100 sentences from each sample\n",
    "\n",
    "Start manual annotation from a smaller subset. Pick 100 samples from each 1000 sentence sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94add01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_A_1000_extended.json\n",
      "Initial sentences:    1000\n",
      "Max sample size:      130\n",
      "Unique samples:       129\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_C_1000_extended.json\n",
      "Initial sentences:    996\n",
      "Max sample size:      130\n",
      "Unique samples:       130\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_D_1000_extended.json\n",
      "Initial sentences:    999\n",
      "Max sample size:      130\n",
      "Unique samples:       125\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_G_1000_extended.json\n",
      "Initial sentences:    998\n",
      "Max sample size:      130\n",
      "Unique samples:       129\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_H_1000_extended.json\n",
      "Initial sentences:    999\n",
      "Max sample size:      130\n",
      "Unique samples:       127\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_I_1000_extended.json\n",
      "Initial sentences:    346\n",
      "Max sample size:      130\n",
      "Unique samples:       126\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_J_1000_extended.json\n",
      "Initial sentences:    1000\n",
      "Max sample size:      130\n",
      "Unique samples:       130\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_K_1000_extended.json\n",
      "Initial sentences:    998\n",
      "Max sample size:      130\n",
      "Unique samples:       127\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_N_1000_extended.json\n",
      "Initial sentences:    984\n",
      "Max sample size:      130\n",
      "Unique samples:       129\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_O_1000_extended.json\n",
      "Initial sentences:    898\n",
      "Max sample size:      130\n",
      "Unique samples:       129\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_P_1000_extended.json\n",
      "Initial sentences:    999\n",
      "Max sample size:      130\n",
      "Unique samples:       130\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_S_1000_extended.json\n",
      "Initial sentences:    1000\n",
      "Max sample size:      130\n",
      "Unique samples:       120\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_U_1000_extended.json\n",
      "Initial sentences:    984\n",
      "Max sample size:      130\n",
      "Unique samples:       129\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_V_1000_extended.json\n",
      "Initial sentences:    997\n",
      "Max sample size:      130\n",
      "Unique samples:       130\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_X_1000_extended.json\n",
      "Initial sentences:    36\n",
      "Max sample size:      36\n",
      "Unique samples:       36\n",
      "Unique samples[final]:36\n",
      "\n",
      "pos_Y_1000_extended.json\n",
      "Initial sentences:    986\n",
      "Max sample size:      130\n",
      "Unique samples:       119\n",
      "Unique samples[final]:100\n",
      "\n",
      "pos_Z_1000_extended.json\n",
      "Initial sentences:    996\n",
      "Max sample size:      130\n",
      "Unique samples:       118\n",
      "Unique samples[final]:100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs on old generated data, which was available\n",
    "input_dir = 'unlabelled/pos_terms_1000_extended'\n",
    "\n",
    "output_dir = 'unlabelled/pos_terms_100_extended'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Important: fix seed\n",
    "rnd = random.Random()\n",
    "rnd.seed(1)\n",
    "for file in os.listdir(input_dir):\n",
    "    print(file)\n",
    "    new_data = []\n",
    "    with open(f\"{input_dir}/{file}\", 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    max_sents = len(data)\n",
    "    print(f'Initial sentences:    {max_sents}')\n",
    "    samples = 130 if max_sents >=130 else max_sents\n",
    "    print(f'Max sample size:      {samples}')\n",
    "    uniqs = rnd.sample(range(0, max_sents), samples)\n",
    "    assert len(list(set(uniqs))) == samples\n",
    "\n",
    "    for i in uniqs:\n",
    "        if len(data[i]['predictions'][0]['result'])==1:\n",
    "            new_data.append(data[i])\n",
    "\n",
    "    print(f'Unique samples:       {len(new_data)}')\n",
    "    print(f'Unique samples[final]:{len(new_data[:100])}')\n",
    "    #break\n",
    "    filename_stub = file.replace(\"_1000_extended.json\", \"\")\n",
    "    output_path = os.path.join(output_dir, f'{filename_stub}_100_extended.json')\n",
    "    json_data = json.dumps(new_data[:100])\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(new_data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f2254",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f109be",
   "metadata": {},
   "source": [
    "**Note**: this notebook contains refactored code for data preparation, but the original input data this code was created for is no longer fully available (due to missing data sampling seed). Thus, the outcomes printed in this notebook do no correspond exactly to outputs of original data preparation notebooks (which are distributed elsewhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184421a8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
