{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a651dd",
   "metadata": {},
   "source": [
    "# Workflow for updating the benchmark\n",
    "\n",
    "This workflow adds texts to the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18418a9f",
   "metadata": {},
   "source": [
    "## I. Initial split  into subpopulations  \n",
    "\n",
    "The entire recall set consists of all geographic named entities for which the last word is among [the predefined suffix_words](geo_terms.txt). \n",
    "\n",
    "\n",
    "|Subpopulation     | Description | Examples |\n",
    "|:--- |:---|:---|\n",
    "|Levinumad         | Geographic locations with most frequent suffixes | Niiluse jõgi, Aasovi meri, Peipsi järv   |  \n",
    "|Mitmetäneduslikud | Geographic locations with ambigous suffixes      | Panama kanal, Panga pank, Kura kurk      |\n",
    "|Ülejäänud         | Other geographic locations                       | Vaikne ookean, Liivi laht, Tehvandi mägi |   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e432b2",
   "metadata": {},
   "source": [
    "## II. Collect and merge annotations. Check consistency with the benchmark setup\n",
    "\n",
    "* Collect manually labelled geo_terms (judgements about whether a term belongs to NE or not);\n",
    "* Collect manually re-annotated geo_term phrases (combinations of automatic NE predictions and manual fixes to get full phrases);\n",
    "* Merge manual annotations into recall_sets CSV files;\n",
    "* Validate that outcome CSV files are readable;\n",
    "* Check that there are no invalid nor duplicate annotations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13867bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['levinumad', 'mitmetahenduslikud', 'ulejaanud'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from helper_functions import load_term_subpopulations\n",
    "term_subpopulations = load_term_subpopulations()\n",
    "term_subpopulations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5326ac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 'levinumad' ...\n",
      "OK\n",
      "Validating 'mitmetahenduslikud' ...\n",
      "OK\n",
      "Validating 'ulejaanud' ...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Validate and collect manually labelled geo_terms (from 1st annotation phase)\n",
    "annotated_terms = {}\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    print(f'Validating {subpopulation!r} ...')\n",
    "    filename = f'labelled_data/substringtagger_labels/koond_1000_{subpopulation}_labelled.json'\n",
    "    # check it can be opened and json loaded\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for task in data:\n",
    "        # we have one annotation per text in this setup\n",
    "        assert len(task['annotations']) == 1\n",
    "        # check that the task has been annotated by someone\n",
    "        assert task['annotations'][0]['completed_by'] > 0\n",
    "        # check that yes or no has been chosen\n",
    "        assert task['annotations'][0]['result'][-1]['value']['choices'][0] in ['yes','no']\n",
    "    print('OK')\n",
    "\n",
    "    # Collect annotations (only positive cases)\n",
    "    if subpopulation not in annotated_terms.keys():\n",
    "        annotated_terms[subpopulation] = []\n",
    "    for task in data:\n",
    "        if task['annotations'][0]['result'][-1]['value']['choices'][0] == 'yes':\n",
    "            # recall set contains of the raw sentence and the tagged 1-word span\n",
    "            annotated_terms[subpopulation].append( [task['data']['text'], task['annotations'][0]['result'][0]['value']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3722805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Kutsume kõiki huvilisi 17. mail kell 11 Hurmi järve äärde külavisiooni talgutele !',\n",
       "  {'start': 46, 'end': 51, 'text': 'järve', 'labels': ['v172_geo_terms']}],\n",
       " ['14. dets. 1936. a. Peipsi järvelt 7 kalurit...milline relvastus meie piirivalvuritel , kas on kordonites kuulipildujaid , kuidas on ülemuste nimed ja aukraadid .',\n",
       "  {'start': 26, 'end': 33, 'text': 'järvelt', 'labels': ['v172_geo_terms']}],\n",
       " ['Ta teenis Aegna saarel ning mängis sõjaväe orkestris .',\n",
       "  {'start': 16, 'end': 22, 'text': 'saarel', 'labels': ['v172_geo_terms']}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only have terms, which need to be extended to NE phrases\n",
    "annotated_terms['levinumad'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95ddb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reannotations of 'levinumad' ...\n",
      "Collecting reannotations of 'mitmetahenduslikud' ...\n",
      "Collecting reannotations of 'ulejaanud' ...\n"
     ]
    }
   ],
   "source": [
    "# Validate and collect manually relabelled geo_terms phrases (from 2nd annotation phase)\n",
    "reannotated_phrases = {}\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    print(f'Collecting reannotations of {subpopulation!r} ...')\n",
    "    with open(f'labelled_data/ner_labels/koond_1000_{subpopulation}_truelabelled.json', 'r', encoding='UTF-8') as in_f:\n",
    "        relabelled = json.load(in_f)\n",
    "    # Validate that the number of relabelled samples matches\n",
    "    # the number of initial (positive) labellings\n",
    "    assert len(relabelled) == len(annotated_terms[subpopulation])\n",
    "    reannotated_phrases[subpopulation] = relabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "167c7fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging 'levinumad' labelling ...\n",
      "Merging 'mitmetahenduslikud' labelling ...\n",
      "Merging 'ulejaanud' labelling ...\n"
     ]
    }
   ],
   "source": [
    "# Merge manual annotations\n",
    "import csv\n",
    "import os, os.path\n",
    "\n",
    "output_dir = 'labelled_data/recall_sets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Manually ruled out location phrases\n",
    "exceptions = {'Ka Pärnu jõkke Reiu jõe suudmealal lasti 120 000 maimu .' : ['Pärnu jõkke Reiu jõe'], \n",
    "              \"Mootorpaat nelja mehega pardal väljus ööl vastu teisipäeva kella kahe ajal Ruhnu saarelt merele , et söita Pärnusse .\" : ['Ruhnu saarelt merele'], \n",
    "              'Põhja-Tallinnas Stroomi randa ning Haaberstis Harku järve ja Kakumäe randadesse .' : ['Haaberstis Harku järve'], \n",
    "              'Kümneid turiste viidi haiglasse ja vähemalt üks hukkus Kreekas Chalkidiki poolsaare kuurortide ümbruses möllavas hiigelmetsapõlengus .' : ['Kreekas Chalkidiki poolsaare'], \n",
    "              'Kaks aastat pärast seda alustas Hawaiil tööd USA Vaikse ookeani hoiatuskeskus .': ['USA Vaikse ookeani'] }\n",
    "\n",
    "subpopulation_csv_data = {}\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    print(f'Merging {subpopulation!r} labelling ...')\n",
    "    csv_data = []\n",
    "    item_id = 0\n",
    "    for first_annotations in annotated_terms[subpopulation]:\n",
    "        sentence_str = first_annotations[0]\n",
    "        sentence_found = False\n",
    "        for reannotation in reannotated_phrases[subpopulation]:\n",
    "            sentence_str_2 = reannotation['data']['text']\n",
    "            if sentence_str == sentence_str_2:\n",
    "                sentence_found = True\n",
    "                term_start = first_annotations[1]['start']\n",
    "                term_end   = first_annotations[1]['end']\n",
    "                for span in reannotation['annotations'][0]['result']:\n",
    "                    span_info = span['value']\n",
    "                    # Locate all NE phrases/spans containing the manually checked geo_term\n",
    "                    if 'start' in span_info and span_info['start'] <= term_start and span_info['end'] >= term_end:\n",
    "                        if span['origin'] == 'manual':\n",
    "                            # Automatic NE tool was unable to detect the phrase,\n",
    "                            # so it was marked manually\n",
    "                            csv_item = {}\n",
    "                            csv_item[''] = item_id\n",
    "                            csv_item['text'] = sentence_str\n",
    "                            csv_item['span'] = span_info\n",
    "                            csv_item['correct'] = 'no'\n",
    "                            csv_data.append( csv_item )\n",
    "                            item_id += 1\n",
    "                        if span['origin'] == 'prediction':\n",
    "                            # Automatically predicted NE phrase overlaps with\n",
    "                            # the manually checked geo_term. \n",
    "                            #\n",
    "                            # We can take out NE phrases with exceptions:\n",
    "                            # 1) leave out automatic 'ORG' annotations;\n",
    "                            # 2) leave out phrases longer than 2 words if\n",
    "                            #    they contain mistakenly joined NE-s.\n",
    "                            #\n",
    "                            # Note: this work was previously done manually  \n",
    "                            # on the output csv file, and needs to be done \n",
    "                            # again from the scratch if dataset gets updated \n",
    "                            # with new samples. Here we just use hardcoded \n",
    "                            # results of manual phrase removal to ensure \n",
    "                            # repeatability.\n",
    "                            is_exception = \\\n",
    "                                (sentence_str in exceptions and \\\n",
    "                                 span_info['text'] in exceptions[sentence_str])\n",
    "                            if span_info['labels'] != ['ORG'] and not is_exception:\n",
    "                                csv_item = {}\n",
    "                                csv_item[''] = item_id\n",
    "                                csv_item['text'] = sentence_str\n",
    "                                csv_item['span'] = span_info\n",
    "                                csv_item['correct'] = 'yes'\n",
    "                                csv_data.append( csv_item )\n",
    "                            item_id += 1\n",
    "                break\n",
    "        if not sentence_found:\n",
    "            raise Exception(f'Sentence {sentence_str!r} was not found in re-annotated phrases of {subpopulation!r}.')\n",
    "    subpopulation_csv_data[subpopulation] = csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dead714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving recall_sets of 'levinumad' ...\n",
      "Saving recall_sets of 'mitmetahenduslikud' ...\n",
      "Saving recall_sets of 'ulejaanud' ...\n"
     ]
    }
   ],
   "source": [
    "# Write recall_sets into files\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    print(f'Saving recall_sets of {subpopulation!r} ...')\n",
    "    output_path = os.path.join(output_dir, f'koond_1000_{subpopulation}.csv')\n",
    "    csv_data = subpopulation_csv_data[subpopulation]\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as out_f:\n",
    "        fieldnames = ['', 'text', 'span', 'correct']\n",
    "        writer = csv.DictWriter(out_f, fieldnames=fieldnames, delimiter=',')\n",
    "        writer.writeheader()\n",
    "        for csv_item in csv_data:\n",
    "            writer.writerow(csv_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad16deb",
   "metadata": {},
   "source": [
    "Finally, validate outcome CSV files and check for annotation duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f8a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 'levinumad' ...\n",
      "OK\n",
      "Validating 'mitmetahenduslikud' ...\n",
      "OK\n",
      "Validating 'ulejaanud' ...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from pandas import read_csv\n",
    "from helper_functions import DuplicatesChecker\n",
    "checker = DuplicatesChecker()\n",
    "\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    print(f'Validating {subpopulation!r} ...')\n",
    "    filename = f'labelled_data/recall_sets/koond_1000_{subpopulation}.csv'\n",
    "    # Validate file\n",
    "    assert os.path.exists(filename), f'(!) Missing file: {filename}'\n",
    "    try:\n",
    "        data = read_csv(filename)\n",
    "    except Exception as csv_parsing_err:\n",
    "        raise ValueError(f'(!) Bad input file format: unable to open {filename!r} as a CSV file: ') from csv_parsing_er\n",
    "    # Validate file's contents \n",
    "    for txt, span in zip(data.text, data.span):\n",
    "        checker.check_for_duplicates(txt, span)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72602ebe",
   "metadata": {},
   "source": [
    "## III. Update the benchmark data\n",
    "\n",
    "### Gather necessary counts\n",
    "\n",
    "First, we need to obtain total term counts in each subpopulation. \n",
    "This information is available in the local database file created by the SpanSampler.\n",
    "Reload the database and get the information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b63e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:storage.py:57: connecting to host: 'postgres.keeleressursid.ee', port: 5432, dbname: 'estonian-text-corpora', user: 'soras'\n",
      "INFO:storage.py:108: schema: 'estonian_text_corpora', temporary: False, role: 'estonian_text_corpora_read'\n",
      "Loaded 63 terms from geo_terms.txt.\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import load_configuration, connect_to_database\n",
    "from helper_functions import load_term_subpopulations, count_terms_by_subpopulations\n",
    "from span_sampler_sqlite3 import SpanSampler\n",
    "\n",
    "config = load_configuration('config\\example_configuration.ini')\n",
    "storage = connect_to_database(config)\n",
    "collection = config['source_database']['collection']\n",
    "collection = storage[collection]\n",
    "\n",
    "sampler = SpanSampler(collection=collection, \n",
    "                      layer=config['source_database']['terms_layer'], \n",
    "                      attribute='lemma', \n",
    "                      termsfile='geo_terms.txt', \n",
    "                      db_file_name=config['local_database']['sqlite_file'], \n",
    "                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51482344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'levinumad': 84822, 'mitmetahenduslikud': 81893, 'ulejaanud': 185897}\n"
     ]
    }
   ],
   "source": [
    "# Load total counts of each subpopulation\n",
    "subpopulation_totals = \\\n",
    "    count_terms_by_subpopulations(sampler, subpopulations_dir='config/subpopulations')\n",
    "print(subpopulation_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75244d1c",
   "metadata": {},
   "source": [
    "Second, get numbers of positive cases (detected entities) for each subpopulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0174cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect numbers of positive cases\n",
    "from pandas import read_csv\n",
    "\n",
    "positives = {}\n",
    "for subpopulation in term_subpopulations.keys():\n",
    "    positives[subpopulation] = 0\n",
    "    filename = f'labelled_data/recall_sets/koond_1000_{subpopulation}.csv'\n",
    "    try:\n",
    "        data = read_csv(filename)\n",
    "    except Exception as csv_parsing_err:\n",
    "        raise ValueError(f'(!) Bad input file format: unable to open {filename!r} as a CSV file: ') from csv_parsing_err\n",
    "    positives[subpopulation] += len(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba43ac",
   "metadata": {},
   "source": [
    "### Create dataset description CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e825df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3193b8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>occurences</th>\n",
       "      <th>labelled</th>\n",
       "      <th>positive</th>\n",
       "      <th>occurence_ratio</th>\n",
       "      <th>detection_ratio</th>\n",
       "      <th>relative_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>levinumad</td>\n",
       "      <td>84822</td>\n",
       "      <td>1000</td>\n",
       "      <td>350</td>\n",
       "      <td>0.240553</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.473287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mitmetahenduslikud</td>\n",
       "      <td>81893</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.232247</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ulejaanud</td>\n",
       "      <td>185897</td>\n",
       "      <td>1000</td>\n",
       "      <td>172</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.509740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           population  occurences  labelled  positive  occurence_ratio  \\\n",
       "0           levinumad       84822      1000       350         0.240553   \n",
       "1  mitmetahenduslikud       81893      1000        13         0.232247   \n",
       "2           ulejaanud      185897      1000       172         0.527200   \n",
       "\n",
       "   detection_ratio  relative_frequency  \n",
       "0            0.350            0.473287  \n",
       "1            0.013            0.016972  \n",
       "2            0.172            0.509740  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add initial statistics about sub populations\n",
    "sorted_populations = sorted(term_subpopulations.keys())\n",
    "df = DataFrame({\n",
    "    'population': sorted_populations,\n",
    "    'occurences': [subpopulation_totals[s_pop] for s_pop in sorted_populations],\n",
    "    'labelled': [1000 for s_pop in sorted_populations],\n",
    "    'positive': [positives[s_pop] for s_pop in sorted_populations]\n",
    "})\n",
    "# Compute some additional statistics\n",
    "df['occurence_ratio'] = df['occurences']/sum(df['occurences'])\n",
    "df['detection_ratio'] = df['positive']/df['labelled']\n",
    "df['relative_frequency'] = df['occurence_ratio'] * df['positive']/sum(df['occurence_ratio'] * df['positive'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfce9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add file names (full paths)\n",
    "df['file'] = [f'amundsen_01/data/recall_sets/koond_1000_{s_pop}.csv' for s_pop in sorted_populations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ac6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv\n",
    "df.to_csv('data_description.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
