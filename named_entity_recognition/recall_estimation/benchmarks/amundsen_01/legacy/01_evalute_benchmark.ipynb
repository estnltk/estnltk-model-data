{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a03d26",
   "metadata": {},
   "source": [
    "# Benchmark evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89788233",
   "metadata": {},
   "source": [
    "## I. Evaluate algorithm to the benchmark data \n",
    "\n",
    "**TODO:** This is not a future-proof way to read in the benchmark data that consists of many files.\n",
    "\n",
    "Creates a list of Text eobjects with gold and predicted layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06eef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text, ElementaryBaseSpan, Layer, Span\n",
    "import ast\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def load_data(datafolder='data',description_file='data_description.csv'):\n",
    "    gold_standard = pd.DataFrame(columns=('text','population'))\n",
    "    desc = read_csv(description_file)\n",
    "    for filename,population in zip(desc.file,desc.population):\n",
    "        data = read_csv(datafolder+'/'+filename)\n",
    "        texts = data.text\n",
    "        for text, span in zip(data.text,data.span):\n",
    "            textobj = Text(text)\n",
    "            new_layer = Layer('spans',attributes=['labels'])\n",
    "            textobj.add_layer(new_layer)\n",
    "            span = ast.literal_eval(span)\n",
    "            new_span = Span(ElementaryBaseSpan(span['start'],span['end']),new_layer)\n",
    "            new_span.add_annotation(span)\n",
    "            textobj.spans.add_span(new_span)\n",
    "            gold_standard.loc[len(gold_standard)] = {'text':textobj,'population':population}\n",
    "            \n",
    "    return gold_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95567b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecbd0a",
   "metadata": {},
   "source": [
    "### I.I. EstNLTK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f2afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 535/535 [00:51<00:00, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from estnltk.taggers import NerTagger\n",
    "from helper_functions import evaluate_benchmark\n",
    "\n",
    "nertagger = NerTagger()\n",
    "results = evaluate_benchmark(gold_standard,nertagger,layer='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49ba3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    344\n",
       "no     191\n",
       "Name: correct, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c190c",
   "metadata": {},
   "source": [
    "### I.II. Kairit model I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d908cb",
   "metadata": {},
   "source": [
    "**TODO:** Clean up the code. I wanto to see the instantiation of Kairits model and then the call to meodel evaluation. Nothing more!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348c2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk_neural.taggers.ner.estbertner_tagger import EstBERTNERTagger\n",
    "\n",
    "kairit_1 = EstBERTNERTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c45e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/535 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 535/535 [01:09<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_benchmark(gold_standard,kairit_1,layer='estbertner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d1bfb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    345\n",
       "no     190\n",
       "Name: correct, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd49a7a",
   "metadata": {},
   "source": [
    "### I.III. Kairit model II\n",
    "\n",
    "**TODO:** Clean up the code. I wanto to see the instantiation of Kairits model and then the call to meodel evaluation. Nothing more! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62dfe896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import get_resource_paths\n",
    "kairit_2 = EstBERTNERTagger(output_layer='estbertner2',words_output_layer='nerwords2',model_location=get_resource_paths(\"estbertner_v2\", only_latest=True, download_missing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e438559e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/535 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 535/535 [01:06<00:00,  8.04it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_benchmark(gold_standard,kairit_2,layer='estbertner2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77e0b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     404\n",
       "yes    131\n",
       "Name: correct, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd825ac",
   "metadata": {},
   "source": [
    "## II. Evaluate results\n",
    "\n",
    "**TODO:** To be cleaned up later!\n",
    "\n",
    "Use EstNLTK to compute recall together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cbd6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = read_csv('data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "425372fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.file = ['recall_sets/koond_1000_levinumad.csv','recall_sets/koond_1000_ulejaanud.csv','recall_sets/koond_1000_mitmetahenduslikud.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "942c1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl[' population'] = ['levinumad','ulejaanud','mitmetahenduslikud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85edfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl[' positives'] = [len(levinumad),len(ulejaanud),len(mitmetahenduslikud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d477cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.to_csv('data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d4bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit binomial distributions for each substample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55db0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast and dirty version\n",
    "# Find standard dev for each fraction estimate\n",
    "# Combine stabndard deviation for the linear combination\n",
    "# Find CI based on this estimate\n",
    "\n",
    "# based on:\n",
    "# https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Standard_error_of_a_proportion_estimation_when_using_weighted_data\n",
    "\n",
    "###levinumad_estnltk\n",
    "#following is based on corpus_statistics.ipynb\n",
    "import numpy as np\n",
    "\n",
    "levinum_weight = 28330.05/ (len(levinumad) * 60629.943)\n",
    "ulejaanud_weight = 31313.804 / (len(ulejaanud) * 60629.943)\n",
    "mitmetahenduslik_weight = 986.089 / (len(mitmetahenduslikud) * 60629.943)\n",
    "\n",
    "weight_vector = [levinum_weight] * len(levinumad) + [ulejaanud_weight] * len(ulejaanud) + [mitmetahenduslik_weight] * len(mitmetahenduslikud)\n",
    "\n",
    "## EstNLTK\n",
    "correct_vector_1 = np.concatenate(((levinumad.estnltk_correct=='yes'),(ulejaanud.estnltk_correct=='yes'),(mitmetahenduslikud.estnltk_correct=='yes')))\n",
    "sample_mean_1 = np.matmul(weight_vector,correct_vector_1)\n",
    "standard_error_1 = np.sqrt(sample_mean_1 * (1-sample_mean_1) * np.sum(np.square(weight_vector)))\n",
    "confidence_interval_1 = (sample_mean_1 - standard_error_1 * 1.96, sample_mean_1 + standard_error_1 * 1.96)\n",
    "\n",
    "### Kairit_1\n",
    "correct_vector_2 = np.concatenate(((levinumad.Kairit_1_correct=='yes'),(ulejaanud.Kairit_1_correct=='yes'),(mitmetahenduslikud.Kairit_1_correct=='yes')))\n",
    "sample_mean_2 = np.matmul(weight_vector,correct_vector_2)\n",
    "standard_error_2 = np.sqrt(sample_mean_2 * (1-sample_mean_2) * np.sum(np.square(weight_vector)))\n",
    "confidence_interval_2 = (sample_mean_2 - standard_error_2 * 1.96, sample_mean_2 + standard_error_2 * 1.96)\n",
    "\n",
    "### Kairit_2\n",
    "correct_vector_3 = np.concatenate(((levinumad.Kairit_2_correct=='yes'),(ulejaanud.Kairit_2_correct=='yes'),(mitmetahenduslikud.Kairit_2_correct=='yes')))\n",
    "sample_mean_3 = np.matmul(weight_vector,correct_vector_3)\n",
    "standard_error_3 = np.sqrt(sample_mean_3 * (1-sample_mean_3) * np.sum(np.square(weight_vector)))\n",
    "confidence_interval_3 = (sample_mean_3 - standard_error_3 * 1.96, sample_mean_3 + standard_error_3 * 1.96)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "770e2c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5875822776214716\n",
      "(0.5423782871699572, 0.6327862680729861)\n"
     ]
    }
   ],
   "source": [
    "print(sample_mean_1)\n",
    "print(confidence_interval_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c783dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6025960637963985\n",
      "(0.5576591752800651, 0.6475329523127319)\n"
     ]
    }
   ],
   "source": [
    "print(sample_mean_2)\n",
    "print(confidence_interval_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fd3a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2761996164172543\n",
      "(0.23514190757483747, 0.31725732525967115)\n"
     ]
    }
   ],
   "source": [
    "print(sample_mean_3)\n",
    "print(confidence_interval_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6892e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = read_csv('leaderboard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "deb9a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = leaderboard.append({\"Algorithm\":\"EstNLTK\",\" Recall\":sample_mean_1,\" Recall-95CI% \":confidence_interval_1},ignore_index=True)\n",
    "leaderboard = leaderboard.append({\"Algorithm\":\"Kairit_I\",\" Recall\":sample_mean_2,\" Recall-95CI% \":confidence_interval_2},ignore_index=True)\n",
    "leaderboard = leaderboard.append({\"Algorithm\":\"Kairit_II\",\" Recall\":sample_mean_3,\" Recall-95CI% \":confidence_interval_3},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da829f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard.to_csv('leaderboard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct and hard version\n",
    "\n",
    "\n",
    "# https://stats.stackexchange.com/questions/485266/better-confidence-intervals-for-weighted-average\n",
    "\n",
    "# find a convolution of distributions \n",
    "# https://medium.com/analytics-vidhya/sum-of-two-random-variables-or-the-rocky-path-to-understanding-convolutions-of-probability-b0fc29aca3b5\n",
    "# https://stackoverflow.com/questions/66218036/question-on-discrete-convolution-with-python\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.convolve.html\n",
    "# https://stackoverflow.com/questions/28901221/faster-convolution-of-probability-density-functions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 95% confidence inteval for that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
