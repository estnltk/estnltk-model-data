{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507defca-8263-4f9f-99cc-ad4c259185f0",
   "metadata": {},
   "source": [
    "### Convert PropBankPreannotator's frames to verb patterns database\n",
    "\n",
    "Convert a subset of PropBankPreannotator's frames lexicon to [verb patterns sqlite3 database](https://github.com/estnltk/syntax_experiments/blob/verb_templates/verb_patterns/vp_data2_documentation/patterns.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4a4fac-4e77-4c5a-a8f5-bf67ff51e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc7046b-ff04-41f5-90e0-76d420830455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new database / connect to an existing one\n",
    "con = sqlite3.connect(\"propbank_preannotator_verb_patterns.db\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9129cc-4f5d-4b9f-b03a-676f660e7f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1729a3297c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new patterns table\n",
    "cur.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS propbank_patterns\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE propbank_patterns (\n",
    "       pat_id integer, \n",
    "       pattern text,\n",
    "       verb_word text, \n",
    "       verb_compound text, \n",
    "       phrase_nr integer, \n",
    "       phrase_case text, \n",
    "       adp text, \n",
    "       inf_verb text);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4dcdac9-471c-48cf-b773-c47f5535c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate PropBankPreannotator's lexicon \n",
    "import os, os.path\n",
    "from estnltk.downloader import get_resource_paths\n",
    "propbank_lexicon_path = None\n",
    "propbank_lexicon_path = 'propbank_frames.jl'\n",
    "if propbank_lexicon_path is None:\n",
    "    # Try to download PropBankPreannotator's lexicon via estnltk's resources\n",
    "    propbank_lexicon_path = get_resource_paths(\"propbankpreannotator\", only_latest=True, download_missing=True)\n",
    "    propbank_lexicon_path = os.path.join(propbank_lexicon, 'propbank_frames.jl') if propbank_lexicon_path is not None else None\n",
    "assert os.path.exists(propbank_lexicon_path), \\\n",
    "    f'(!) Illegal path for propbank lexicon: {propbank_lexicon_path}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1d37c2-f3ba-453a-8b53-467061e08f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 714 frame entries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load entries from PropBankPreannotator's lexicon\n",
    "import json\n",
    "entries_loaded = 0\n",
    "frame_lexicon = {}\n",
    "with open(propbank_lexicon_path, 'r', encoding='utf-8') as in_f:\n",
    "    for entry in in_f:\n",
    "        #\n",
    "        # Example entry:\n",
    "        #\n",
    "        #  {\"sense_id\": \"eitama_1\", \n",
    "        #   \"lemma\": \"eitama\", \n",
    "        #   \"class\": \"KÕNEAKT\", \n",
    "        #   \"description\": \"\", \n",
    "        #   \"complete\": true, \n",
    "        #   \"arguments\": [{\"name\": \"Arg0\", \"description\": \"eitaja\", \"variants\": [{\"feats\": [\"deprel=nsubj\"]}]}, \n",
    "        #                 {\"name\": \"Arg1\", \"description\": \"seda\", \"variants\": [{\"feats\": [\"deprel=obj\"]}]}]}\n",
    "        #\n",
    "        entry = entry.strip()\n",
    "        if entry.startswith('#'):\n",
    "            # Skip comment lines\n",
    "            continue\n",
    "        entry_dict = json.loads(entry)\n",
    "        lemma = entry_dict['lemma']\n",
    "        if lemma not in frame_lexicon:\n",
    "            frame_lexicon[lemma] = []\n",
    "        frame_lexicon[lemma].append( entry_dict )\n",
    "        entries_loaded += 1\n",
    "print(f'Total {entries_loaded} frame entries loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed617bb3-beb6-421b-a102-e05bbac545fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping cases from UD lowercase to morph_extended\n",
    "ud_to_morph_ext_case_mapping = {\n",
    "    'nom': 'nom', \n",
    "    'gen': 'gen',\n",
    "    'par': 'part',\n",
    "    'ill': 'ill',\n",
    "    'ine': 'in',\n",
    "    'ela': 'el',\n",
    "    'all': 'all', \n",
    "    'ade': 'ad',\n",
    "    'abl': 'abl',\n",
    "    'tra': 'tr',\n",
    "    'ter': 'term',\n",
    "    'ess': 'es',\n",
    "    'abe': 'abes',\n",
    "    'com': 'kom',\n",
    "    # aditiiv\n",
    "    'add': 'adit'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9bad6-6cb6-49a4-b1de-1fc76741d197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted argument descriptions: 626 / 2102\n"
     ]
    }
   ],
   "source": [
    "def extract_feats( feats:str ):\n",
    "    extracted_feats = {}\n",
    "    for feat_str in feats:\n",
    "        assert '=' in feat_str\n",
    "        fname, fval = feat_str.split('=')\n",
    "        if fname == 'case':\n",
    "            me_case = ud_to_morph_ext_case_mapping.get(fval.lower(), '<missing>')\n",
    "            extracted_feats['phrase_case'] = me_case\n",
    "        elif fname == 'casemarker':\n",
    "            extracted_feats['adp'] = fval.lower()\n",
    "    return extracted_feats\n",
    "\n",
    "\n",
    "\n",
    "def insert_into_db( connection, cursor, feats_dict, table='propbank_patterns', missing_placeholder='' ):\n",
    "    # Placeholder for missing values\n",
    "    #missing_placeholder = None\n",
    "    columns = ['pat_id', 'pattern', 'verb_word', 'verb_compound', 'phrase_nr', 'phrase_case', 'adp', 'inf_verb']\n",
    "    values = []\n",
    "    for col in columns:\n",
    "        v = feats_dict.get(col, missing_placeholder)\n",
    "        values.append( v )\n",
    "    cursor.executemany(f\"INSERT INTO {table} ({','.join(columns)}) VALUES ({','.join(['?' for c in columns])})\", \\\n",
    "                                                                                     [tuple(values)] )\n",
    "    connection.commit()\n",
    "\n",
    "cur_pat_id = 0\n",
    "extracted_args = 0\n",
    "discarded_args = 0\n",
    "for verb_lemma in frame_lexicon.keys():\n",
    "    for frame in frame_lexicon[verb_lemma]:\n",
    "        collected_patterns = []\n",
    "        phrase_nr = 1\n",
    "        for arg in frame[\"arguments\"]:\n",
    "            # Different variants features can be used to describe a single argument, e.g.\n",
    "            #\n",
    "            #  rääkima  {'name': 'Arg3', \n",
    "            #            'description': 'millegi kohta/millest', \n",
    "            #            'variants': [{'feats': ['deprel=obl', 'case=Gen', 'casemarker=kohta']}, \n",
    "            #                         {'feats': ['deprel=obl', 'case=Ela']}]}\n",
    "            #\n",
    "            arg_extracted = False\n",
    "            for variant in arg['variants']:\n",
    "                feats_dict = extract_feats(variant['feats'])\n",
    "                if len(feats_dict.keys()) > 0:\n",
    "                    arg_name = arg['name']\n",
    "                    feats_dict['pattern'] = f'{frame[\"sense_id\"]} {arg_name}'\n",
    "                    arg_description = arg.get('description', None)\n",
    "                    if arg_description is not None and len(arg_description) > 0:\n",
    "                        feats_dict['pattern'] += f' ({arg_description.strip(\"()\")})'\n",
    "                    feats_dict['phrase_nr'] = phrase_nr\n",
    "                    collected_patterns.append( feats_dict )\n",
    "                    arg_extracted = True\n",
    "                else:\n",
    "                    discarded_args += 1\n",
    "            if arg_extracted:\n",
    "                phrase_nr += 1\n",
    "        if collected_patterns:\n",
    "            # Finalize patterns: all argument descriptions belonging to the same \n",
    "            # frame will get the same pat_id. Add verb lemmas\n",
    "            for feats_dict in collected_patterns:\n",
    "                feats_dict['verb_word'] = verb_lemma\n",
    "                feats_dict['pat_id'] = cur_pat_id\n",
    "                extracted_args += 1\n",
    "                #print( feats_dict )\n",
    "            # Insert collected patterns to db\n",
    "            for feats_dict in collected_patterns:\n",
    "                insert_into_db( con, cur, feats_dict, table='propbank_patterns' )\n",
    "            cur_pat_id += 1\n",
    "print()\n",
    "print(f'Extracted argument descriptions: {extracted_args} / {extracted_args+discarded_args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7498409c-05ca-4478-a572-e587bb37a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ühenduse sulgemine\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
